1. 표준화(standardization) = stadardscaler

각 observation이 평균을 기준으로 어느 정도 떨어져 있는지를 나타낼때 사용된다. 값의 스케일이 다른 두 개의 변수가 있을 때, 
이 변수들의 스케일 차이를 제거해 주는 효과가 있다.
제로 평균 으로부터 각 값들의 분산을 나타낸다.
각 요소의 값에서 평균을 뺀 다음 표준편차로 나누어 준다.

standardization은 표준화이다. 고등학교때 배운 표준정규분포를 구하는 식으로 구하면 된다.
다른 말로는 z - transformation이라고도 하고, 그렇게 표준화된 값을 z - score라고도 한다. 
standardization을 통해 KNN에서 해본 wine classification을 해보면 94%정도의 정확도가 나온다. 
다시한번 말하지만 물론 KNN말고도 일반적인 데이터 전처리에 사용된다. 
X = X-Xmean/Xstd
mean 차감을 통해 zero-centered화를 시켜주고 std로 나누어줌으로써 데이터가 일정 범위 안에 머무르게 합니다.
cs231n에 따르면 평균, 표준편차는 train set의 값을 써야한다는 점입니다. 즉, 데이터셋을 train,val,test로 나눈 뒤, train set의 
mean, std를 구하여 val, test set 각각에 적용시켜주어야 합니다. 

표준화는 유사한 목표를 달성하기 위해 설계되었으며, 서로 비슷한 범위를 가지며 프로그래머가 원시 

데이터에서 단서를 얻을 수 있도록 데이터 분석에 널리 사용되는 기능을 생성합니다.

대수학에서 정규화는 벡터를 길이로 나눈 값을 말하는 것으로 보이며 데이터를 0과 1 사이의 범위로 변환합니다

그리고 통계에서 표준화는 평균을 뺀 다음 SD (표준 편차)로 나눈 것입니다. 표준화는 결과 분포가 

평균 0과 표준 편차 1을 갖도록 데이터를 변환합니다

 2. 정규화(normalization) = minmax
정규화는 데이터의 범위를 0과 1로 변환하여 데이터 분포를 조정하는 방법이다. 
(해당 값- 최소값) / (최대값-최소값) 을 해주면 된다.

조금 헷갈리는게, 표준화와 정규화가 약간 혼용되어 사용된다는 점이다. 일단 구글링의 결과 정규화는 
우리가 고등학교때 배운 정규분포의 정규화가 아니라 데이터의 범주를 바꾸는 작업이다.
예를 들어 여러개의 feature가 있는데 어떤거는 100 ~ 200사이의 값이고, 어떤거는 -10 ~ 10 사이의 값이고, 
어떤거는 -100 ~ 300 사이의 값이라고 해보자.
그러면 이 값들을 분석하기 쉽지가 않을 것이다. 따라서 이런 불편을 줄이기 위해 범주를 일치시킬 수 있다. 
이런 작업을 normalization이라고 한다. 보통은 0 ~ 1 사이의 값으로 범주를 일치시킨다. 
normalize하면 0 ~ 1로 범주가 일치된다.

X= X-Xmin/Xmax-Xmin

정규화는 언더 피트 (underfit)와 오버 피트 (overfit)의 집중이다.

기차 데이터와 테스트 데이터에서 오류가 더 많이 발생하면 그 underfit

오류가 테스트 데이터에 더 많고 기차 데이터가 적 으면 overfit입니다.

정규화는 최적의 오류를 관리하는 방법입니다
정규화는 기계 학습 알고리즘을 학습 할 때 과도한 작업 을 피하는 기술 입니다. 
모델이 지나치게 적합하다면 모델은 정확도가 낮을 ​​것이고,
이 정규화를 극복하기 위해서는 모델을 제로쪽으로 추정하는 계수를 제한하고 정규화함으로써 
모델을 얻을 수 있습니다.

일반화 Regularization

Regularization은 말 그대로 제약이다.

기계 학습에서 무엇을 왜 어떻게 제약하는지에 대해서 알아보자.
직관적으로 생각하면, 어떤 문제를 해결하는 분류기를 찾고 싶을 때 해당 분류기가
들어있는 통이 너무 커서 적절한 답을 찾기 힘들 때 분류기들을 일단 채에 한번 걸른 후에
걸러진 것들 중에 답을 찾는 것과 비슷하다. 채의 구멍의 크기 혹은 모양이 서로 다른
Regularizationd을 의미
수학적으로는 들어오는 데이터 y=f+입실론sms error 입실론이 들어가 있다.
그래서 모델은 에러를 같이 학습하려는 경향이 있다. 그래서 in-sampleerror:E-in은 
좋게 나올 수 있으나 out-sample error: E-out은 나빠지는 현상이 발생
단순히 E-in을 줄이는 것 뿐 아니라 특정 제약 조건, Regularization term, 을 
추가해서 Regularization property를 좋게 하는데 그 목적이 있다.

즉 에러나 model complexity 등에 인한 문제로 overfit되는 것을 막아주는 것을 알 수 있다.

보통 번역은 '정규화' 라고 하지만 '일반화' 라고 하는 것이 이해에는 더 도움이 될 수도 있습니다. 
모델 복잡도에 대한 패널티로 정규화는 Overfitting 을 예방하고 Generalization(일반화) 성능을 높이는데 도움을 줍니다.
Regularization 방법으로는 L1 Regularization, L2 Regularization, Dropout, Early stopping 등이 있습니다.


Overfitting
Overfitting은 훈련 데이터에만 지나치게 적응하여 시험 데이터에 제대로 반응하지
못하는 현상을 말한다.
- 매개변수가 많고 표현력이 높은 모델인 경우
- 훈련 데이터가 적은 경우

Regularization
Overfitting을 억제하기 위해서 다음과 같은 Regularization기법을 사용

weight decay(L2 Regularization)
L2 Regularization은 가장 일반적으로 사용되는 Regularization기법으로, Overfitting은
가중치 매개변수의 값이 커서 발생하는 경우가 많기 때문에 가중치가 클 수록 큰 페널티를 
부과하여 Overfitting을 억제하는 방법

Dropout
다른 Regularization 기법들과 (L1,L2,maxnorm) 상호 보완적인 방법으로 알려져 있다
Dropout은 각 계층 마다 일정 비율의 뉴런을 임의로 정해 drop시켜 나머지 뉴런들만
학습하도록 하는 방법
이는 네트워크 내부에서 이루어지는 ensemble learning이라 생각해도 좋다.

* 앙상블 학습은 개별적으로 학습시킨 여러 모델의 출력을 종합해 추론하는 방식이다
Dropout과 별개로 실행 시점에 따로 적용할 수 있다. 뉴럴 네트워크를 개별적으로 학습시키고,
각 네트워크에 같은 input을 주어 나온 출력 뉴런 각각의 평균을 구한 다음 여기서 가장 큰
값을 정답으로 판정하면 되기 때문에 간단히 구현할 수 있다.
e.g)(network1.output[1] + network2.output[1])/ 2 = ensemble_output[1]
역전파는 ReLU 처럼 동작한다. 순전파 때 신호를 통과시킨 뉴런은 역전파 때도 그대로
통과시키고, drop된 뉴런은 역전파 때도 신호를 차단한다.
단 시험 때